{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jZqDkc6ZYBO"
      },
      "source": [
        "Step 1: Creating Tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojNlEYJlZQFd",
        "outputId": "3fb8f6e3-d088-4839-e61b-d437a89be779"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total num of character: 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ],
      "source": [
        "with open(\"the-verdict.txt\",\"r\",encoding=\"utf-8\") as f:   # r is read\n",
        "    raw_text = f.read()\n",
        "print(\"total num of character:\", len(raw_text))\n",
        "print(raw_text[:99])   #print 1st 100 characters of the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrC4ZvkmZzuP",
        "outputId": "fbcd2505-55cc-4909-c157-4ab48b935afb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'text.']\n"
          ]
        }
      ],
      "source": [
        "# split text to obtain a list of token\n",
        "import re    # regular expression -> split based on white spaces or any other character\n",
        "\n",
        "text = \"hello, world. This, is a text.\"\n",
        "result = re.split(r'(\\s)',text) #\\s splits where white spaces r encountered\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNIBr6yKbt3o",
        "outputId": "205e5050-7a4a-4ee2-bfff-ca815a7e5ebe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', ' world', ' This', ' is a text', '']\n"
          ]
        }
      ],
      "source": [
        "result = re.split(r'[,.]',text)  #. and , are seperate tokens\n",
        "print(result)\n",
        "# another issue is it still contains white space character which is still counted as tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4Qr0vUIc5NP",
        "outputId": "bb283304-339a-4c62-a1c8-6b925c932d8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', ' world', ' This', ' is a text']\n"
          ]
        }
      ],
      "source": [
        "result = [item for item in result if item.strip()]  #strip cut off white spaces\n",
        "print(result)\n",
        "# keeping white spaces is meaningful (eg: python code as dataset), we are removing just for memory advantages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDkdEBYude5U",
        "outputId": "fa2d6f6a-304e-46b7-a8b2-d0289e1bce64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
          ]
        }
      ],
      "source": [
        "text = \"hello, world. Is this-- a test?\"\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)',text)   # all are seperate tokens now\n",
        "result = [item.strip() for item in result if item.strip()]\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0zzt_Ave6hU",
        "outputId": "2b510ce4-8749-424e-af86-6f40503c55b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
          ]
        }
      ],
      "source": [
        "# convert the entire broad text to individual tokens\n",
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)',raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:30])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ki-VOcvefriA",
        "outputId": "06d8f9f8-dbbf-4d8e-e969-ffc79c4a1ff0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4690\n"
          ]
        }
      ],
      "source": [
        "print(len(preprocessed))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqxJIPeaf4E5"
      },
      "source": [
        "Step 2: Converting tokens to token id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4bGwVhvfvmD",
        "outputId": "53cde001-4b7a-4b50-f53a-5d03b967ee63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1130\n"
          ]
        }
      ],
      "source": [
        "# vocabulary contains unquie tokens\n",
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1evJjTjMgf8r"
      },
      "outputs": [],
      "source": [
        "# creating vocabulary (every token needs to be assign with token id)\n",
        "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
        "# assign token to int values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20wZkLpZhGaJ",
        "outputId": "21f2eb7f-18cf-4fbd-ef4a-23ec3a838af1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "(':', 8)\n",
            "(';', 9)\n",
            "('?', 10)\n",
            "('A', 11)\n",
            "('Ah', 12)\n",
            "('Among', 13)\n",
            "('And', 14)\n",
            "('Are', 15)\n",
            "('Arrt', 16)\n",
            "('As', 17)\n",
            "('At', 18)\n",
            "('Be', 19)\n",
            "('Begin', 20)\n",
            "('Burlington', 21)\n",
            "('But', 22)\n",
            "('By', 23)\n",
            "('Carlo', 24)\n",
            "('Chicago', 25)\n",
            "('Claude', 26)\n",
            "('Come', 27)\n",
            "('Croft', 28)\n",
            "('Destroyed', 29)\n",
            "('Devonshire', 30)\n",
            "('Don', 31)\n",
            "('Dubarry', 32)\n",
            "('Emperors', 33)\n",
            "('Florence', 34)\n",
            "('For', 35)\n",
            "('Gallery', 36)\n",
            "('Gideon', 37)\n",
            "('Gisburn', 38)\n",
            "('Gisburns', 39)\n",
            "('Grafton', 40)\n",
            "('Greek', 41)\n",
            "('Grindle', 42)\n",
            "('Grindles', 43)\n",
            "('HAD', 44)\n",
            "('Had', 45)\n",
            "('Hang', 46)\n",
            "('Has', 47)\n",
            "('He', 48)\n",
            "('Her', 49)\n",
            "('Hermia', 50)\n"
          ]
        }
      ],
      "source": [
        "for i, item in enumerate(vocab.items()):  #enumerate takes all the words and assign an integer to each word in alphabetical order\n",
        "  print(item)\n",
        "  if i >= 50:\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jADz9we7iJZP"
      },
      "source": [
        "encode method = sample text -> tokenized text -> token ids\n",
        "\n",
        "decode method = token ids -> tokenized text -> sample text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2qxHD6HhSzz"
      },
      "outputs": [],
      "source": [
        "class SimpletokenizerV1:\n",
        "  def __init__(self,vocab):\n",
        "    self.str_to_int = vocab  # encode\n",
        "    self.int_to_str = {i:s for s, i in vocab.items()}  #s=token, i=token id #decode\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)',text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \" \".join([self.int_to_str[id] for id in ids])\n",
        "    # Replace spaces before the specified punctuations\n",
        "    text = re.sub(r'\\s+([,.:?!\"()\\'])', r'\\1', text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWd3gkWKjHT3",
        "outputId": "37542bfe-7e79-452e-87eb-f8bb453334c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
          ]
        }
      ],
      "source": [
        "# eg for encoder\n",
        "tokenizer = SimpletokenizerV1(vocab)\n",
        "\n",
        "text = \"\"\"\"It's the last he painted, you know,\"\n",
        "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LVJi9WcWlMKk",
        "outputId": "ab94feea-364c-4ac8-f8bd-3c473d2a4de4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# eg for decoder\n",
        "tokenizer.decode(ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "OWeXxEKilZkR",
        "outputId": "62c441cc-c20e-490f-d09d-ee9d6d2cd3ed"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Hello'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-5e7d869b87b5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Hello, Tea is good?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# gave error as this is not included in the given dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-5649484a9da5>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'([,.:;?_!\"()\\']|--|\\s)'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-5649484a9da5>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'([,.:;?_!\"()\\']|--|\\s)'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Hello'"
          ]
        }
      ],
      "source": [
        "text = \"Hello, Tea is good?\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)\n",
        "# gave error as this is not included in the given dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGNpZcemlvaI"
      },
      "outputs": [],
      "source": [
        "# Adding special context tokens\n",
        "# will modify tokenizer to handle unknown words., and implement a class SimpleTokenizerV2, to support new tokens\n",
        "# V2 -> version 2\n",
        "\n",
        "# at the end of text add <unk>  and  <endoftext>\n",
        "# if we've a sentence in which last word is not in text then it will take the id of lunk\n",
        "# when we're working with multiple text sources then we use endoftext\n",
        "# when 1st text ends endoftext lg jata h then 2nd text end phr lg jata h likewise.. => this leads to more effective processing in llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1um0-AuGCgw"
      },
      "outputs": [],
      "source": [
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|endoftext|>\",\"<|unk|>\"])   # adding two tokens at last\n",
        "vocab = {token:integer for integer, token in enumerate(all_tokens)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HOjT_2eG6Vu",
        "outputId": "f3cb5e5e-df1d-42bf-8612-8ae5b81c164b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1132"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "len(vocab.items())   #length increased by 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Cf0y601G83c",
        "outputId": "921474b2-9375-4750-dd59-54cf028c610d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('younger', 1127)\n",
            "('your', 1128)\n",
            "('yourself', 1129)\n",
            "('<|endoftext|>', 1130)\n",
            "('<|unk|>', 1131)\n"
          ]
        }
      ],
      "source": [
        "for i, item in enumerate(list(vocab.items())[-5:]):  #last 5 entries\n",
        "  print(item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4LMEQDIHPrl"
      },
      "outputs": [],
      "source": [
        "# simple change in this version 2 tokenizer is adding unknown if word not present\n",
        "class SimpletokenizerV2:\n",
        "  def __init__(self,vocab):\n",
        "    self.str_to_int = vocab  # encode\n",
        "    self.int_to_str = {i:s for s, i in vocab.items()}  #s=token, i=token id #decode\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)',text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    preprocessed = [\n",
        "        item if item in self.str_to_int    # if particular entry is not present in the vocabulary, the token assigned to that entry is unknown\n",
        "        else \"<|unk|>\"\n",
        "        for item in preprocessed\n",
        "    ]\n",
        "\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self,ids):\n",
        "    text = \" \".join([self.int_to_str[id] for id in ids])\n",
        "    # Replace spaces before the specified punctuations\n",
        "    text = re.sub(r'\\s+([,.:?!\"()\\'])', r'\\1', text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1-rl_cZI8RJ",
        "outputId": "5b1880b3-8a3c-496c-a4e8-abcacff4b363"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = SimpletokenizerV2(vocab)\n",
        "\n",
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvQ8wtRkJqJK",
        "outputId": "1a58ad9c-4ebd-4cf1-a4cb-22aa1cb2c696"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "tokenizer.encode(text)  # earlier we were getting error as hello was not present in the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aDbyvmibLKZM",
        "outputId": "c677d20b-82c9-469e-bf09-ab527bc9710e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "tokenizer.decode(tokenizer.encode(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aAxSJUzN9Pu"
      },
      "source": [
        "There are also some special tokens present-\n",
        "\n",
        "[BOS] beginning of sequence: marks the start of a text, where a piece of context begins\n",
        "\n",
        "[EOS] end of sentence: positioned at the of text, specially useful when concatenating multiple unrelated texts, similar to <|endoftext|>\n",
        "\n",
        "[PAD] padding: to ensure same length to all text, shorter text are padded to normal length.\n",
        "\n",
        "GPT just uses <|endoftext|> for simplicity\n",
        "\n",
        "GPT doen't uses unk for unknown token, instead it uses byte pair encoding [BPE] tokenizer, which breaks down into subword units. eg: chased a word can be broken down into ch,as,ed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnWbGzUfRlsf"
      },
      "source": [
        "Byte Pair Encoding (BPE)\n",
        "\n",
        "- comes under subword based tokenization\n",
        "\n",
        "The BPE tokenizer covered in this section was used to train LLMs such as GPT-2, GPT-3, and the original model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbIzuXebNn96",
        "outputId": "630323d1-6ac9-4964-967b-ea974bdf078f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n",
            "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n"
          ]
        }
      ],
      "source": [
        "# since implementing BPE can be relatively complicated, we'll use an Python open-source lib called tiktoken\n",
        "# it implements BPE algo very efficiently based on source code in Rust\n",
        "# tiktoken is a fast BPE tokenizer for use with OpenAI's models\n",
        "\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rf_2-NNbeZP",
        "outputId": "7816f071-ff65-4da0-cc3f-0cc70b011339"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version: 0.8.0\n"
          ]
        }
      ],
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "\n",
        "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5W0BHWQibsjv"
      },
      "outputs": [],
      "source": [
        "# we can instantiate BPE tokenizer from tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")   # it is like the one we implemented earlier SimpleTokenizerV2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nl6XPlDKb8Ct",
        "outputId": "d4814758-ebb8-41bf-a17a-5b720e13e91d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
          ]
        }
      ],
      "source": [
        "text = (\n",
        "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
        "    \"of someunknownPlace.\"\n",
        ")\n",
        "# in word level tokenizer \"someunknown\" has given an error until unk was not given but here as it tokenize subword also it will be tokenized and no error\n",
        "\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(integers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xkz4P9xzcxyy",
        "outputId": "c8e23240-0eb7-4604-b693-8f009927dc29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
          ]
        }
      ],
      "source": [
        "# converting token ids back to text using decode method\n",
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BesxEtWTdx_u"
      },
      "source": [
        "observations-\n",
        "\n",
        "<|endoftext|> token is assigned a relatively large token ID, namely 50256\n",
        "\n",
        "In fact, the BPE tokenizer, which was used to train models such as GPT-2, GPT-3, and the original chatgpt, has a total vocab size of 50,257, with <|endoftext|> being assigned the largest token ID.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYf3q9VWdrYw",
        "outputId": "42c7aec9-2b8a-4fc0-b117-4ddb56fefa97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[33901, 86, 343, 86, 220, 959]\n",
            "Akwirw ier\n"
          ]
        }
      ],
      "source": [
        "# eg of how bpe tokenizer deals with unknown tokens\n",
        "integers = tokenizer.encode(\"Akwirw ier\")\n",
        "print(integers)\n",
        "\n",
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liCrz617tA9X"
      },
      "source": [
        "Creating I/P-Target Pairs\n",
        "\n",
        "Implement a data loader that fetches the i/p target pairs using a sliding window."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJp8GwG5fNXr",
        "outputId": "c4953792-0fa2-43a9-c8d9-82938f9ec3ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5145\n"
          ]
        }
      ],
      "source": [
        "# Using BPE tokenizer\n",
        "with open(\"the-verdict.txt\",\"r\",encoding=\"utf-8\") as f:   # r is read\n",
        "    raw_text = f.read()\n",
        "\n",
        "enc_text = tokenizer.encode(raw_text)  #encoding the entire raw text\n",
        "print(len(enc_text))\n",
        "# total no. of tokens in the training test => 5145"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvhjSuodu84_"
      },
      "outputs": [],
      "source": [
        "# removing first 50 tokens from dataset as it results in slightly more interesting text passage\n",
        "enc_sample = enc_text[50:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fwe9ENavu3J",
        "outputId": "a09407f5-43cb-4c37-8793-2f947a00cccb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [290, 4920, 2241, 287]\n",
            "y:     [4920, 2241, 287, 257]\n"
          ]
        }
      ],
      "source": [
        "# one of the easiest & most intuitive way to create ip-target pairs for the next word prediction task is to\n",
        "# create two variables x and y, where x contains i/p tokens and y contains targets\n",
        "\n",
        "context_size = 4 # this can be any num,means model is trained to look sequence of 4 words\n",
        "# [1,2,3,4] target y is next 4 [2,3,4,5]   for 1 -> 2, for 1,2 -> 3, for 1,2,3 -> 4, for 1,2,3,4 -> 5\n",
        "\n",
        "x = enc_sample[:context_size]   #contains the token id of encoded dataset\n",
        "y = enc_sample[1:context_size+1]  #then shift the x array by 1\n",
        "\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y:     {y}\")\n",
        "# if i/p -> 290 then o/p -> 4920, if i/p -> 290,4920 then o/p -> 2241 and likewise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbgNW0y02L3o",
        "outputId": "080089a7-abf1-4aac-de45-8acd8dac952a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[290] -----> 4920\n",
            "[290, 4920] -----> 2241\n",
            "[290, 4920, 2241] -----> 287\n",
            "[290, 4920, 2241, 287] -----> 257\n"
          ]
        }
      ],
      "source": [
        "for i in range(1, context_size+1):   # 1 to 5\n",
        "  context = enc_sample[:i]\n",
        "  desired = enc_sample[i]\n",
        "\n",
        "  print(context, \"----->\", desired)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYEt8G9k3SgK",
        "outputId": "7095c5b8-deac-4f86-a0a2-e48d730f6a8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " and ----->  established\n",
            " and established ----->  himself\n",
            " and established himself ----->  in\n",
            " and established himself in ----->  a\n"
          ]
        }
      ],
      "source": [
        "for i in range(1, context_size+1):   # 1 to 5\n",
        "  context = enc_sample[:i]\n",
        "  desired = enc_sample[i]\n",
        "\n",
        "  print(tokenizer.decode(context), \"----->\", tokenizer.decode([desired]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziDpIPP2V_Fr"
      },
      "source": [
        "FOR efficient data loader implementation, we'll use Pytorch's built-in Dataset and DataLoader classes.\n",
        "\n",
        "step 1: tokenize the entire text\n",
        "\n",
        "step 2: use a sliding window to chunk the book into converting sequences of max_length\n",
        "\n",
        "step 3: return total num of rows in the dataset\n",
        "\n",
        "step 4: return a single row from the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_Jy4OaQUYhO"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDataset(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):   # we need stride to know how much to slide when to create the next i/p, o/p batch\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # use a sliding window to chunk the book  into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1:i + max_length + 1]\n",
        "\n",
        "            self.input_ids.append(input_chunk)\n",
        "            self.target_ids.append(target_chunk)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):        # the data loader will look at getitem method then it'll create the i/p o/p pair\n",
        "        return self.input_ids[idx], self.target_ids[idx]   # based on idx provides that particular row of i/p o/p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhfAGbUwZZob"
      },
      "outputs": [],
      "source": [
        "# now dataloader comes to picture => it help us to do parallel processing\n",
        "# batch size means how many cpu processors we want to run parallely\n",
        "# stride is when we create i/p o/p batches how much we need to skip before we create a next batch\n",
        "# num_of_workers is how many threads u want to split the code on cpu for let's say ||rl processing\n",
        "def create_data_loader_v1(txt, tokenizer, batch_size=4, max_length=256,\n",
        "                          stride=128, shuffle=True, drop_last=True,   # drop last=true drops the last batch if it is shorter than the specified batch_size to prevent loss spikes during training\n",
        "                          num_workers=0):\n",
        "    # initialize the tokenizer\n",
        "    # tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # create the dataset\n",
        "    dataset = GPTDataset(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # create dataloader\n",
        "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle,\n",
        "                             drop_last=drop_last, num_workers=num_workers)\n",
        "\n",
        "    return data_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xihVyfthcAe-"
      },
      "outputs": [],
      "source": [
        "# now we'll test the dataloader with a batch size of 1 for an LLM with a context size of 4,\n",
        "# This will develop an intuition of how the GPTDatasetV1 class and the create_dataloader_v1 func work together\n",
        "\n",
        "with open(\"the-verdict.txt\",\"r\",encoding=\"utf-8\") as f:   # r is read\n",
        "    raw_text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PrcYsqfDSj9",
        "outputId": "58ef1868-1649-4b00-bd2f-cfb1c3bb57ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.5.1+cu124\n",
            "[[tensor([40]), tensor([367]), tensor([2885]), tensor([1464])], [tensor([367]), tensor([2885]), tensor([1464]), tensor([1807])]]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import tiktoken\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "dataloader = create_data_loader_v1(\n",
        "    raw_text, tokenizer, batch_size=1, max_length=4, stride=1,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "\n",
        "print(first_batch)\n",
        "# i/p tensor and the o/p tensor\n",
        "# since max_length is set to 4 each of the two tensors contains 4 token IDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIYzzIZ8G0zo",
        "outputId": "b2b39683-0c49-4011-af9d-ab842d125d03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[tensor([367]), tensor([2885]), tensor([1464]), tensor([1807])], [tensor([2885]), tensor([1464]), tensor([1807]), tensor([3619])]]\n"
          ]
        }
      ],
      "source": [
        "second_batch = next(data_iter)\n",
        "print(second_batch)\n",
        "# shifted by 1 place (sliding window approach)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhE9OHx2JvmP",
        "outputId": "1962588c-1a80-4b6c-dbc3-4af640c48f5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs: [tensor([   13,   621,  4808, 41379,   887,  6777,   326,    11]), tensor([  198,   611,  5562,   293,   345,    13, 11542,   290]), tensor([198, 314,  62, 373, 655, 632, 373, 287]), tensor([3347, 1549,  465,   11,  531,  373, 1813,  465])]\n",
            "targets: [tensor([  198,   611,  5562,   293,   345,    13, 11542,   290]), tensor([198, 314,  62, 373, 655, 632, 373, 287]), tensor([3347, 1549,  465,   11,  531,  373, 1813,  465]), tensor([4376, 1239, 2106,  287,  438,  407,  502,  898])]\n"
          ]
        }
      ],
      "source": [
        "dataloader = create_data_loader_v1(raw_text, tokenizer, batch_size=8, max_length=4, stride=1)\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "\n",
        "print(\"inputs:\", inputs)\n",
        "print(\"targets:\", targets)\n",
        "# batch size is 8 so i/p & o/p tensor has 8 i/ps, o/ps\n",
        "# also not overlappin as stride is 4  //prevents overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8E9trbPwOLe8"
      },
      "source": [
        "Toy demo of vector embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GmOvhjpMAOh",
        "outputId": "a6958937-527c-4469-c197-980dc982f3af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
          ]
        }
      ],
      "source": [
        "# import trained model\n",
        "import gensim.downloader as api\n",
        "model = api.load(\"glove-wiki-gigaword-300\")   # this model can take any word as i/p and convert it to vectors(300 dimensional vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjmyWlkWOpRN",
        "outputId": "87083651-9815-4dc8-b447-0ae60782a062"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-2.7628e-01  1.3999e-01  9.8519e-02 -6.4019e-01  3.1988e-02  1.0066e-01\n",
            " -1.8673e-01 -3.7129e-01  5.9740e-01 -2.0405e+00  2.2368e-01 -2.6314e-02\n",
            "  7.2408e-01 -4.3829e-01  4.8886e-01 -3.5486e-03 -1.0006e-01 -3.0587e-01\n",
            " -1.5621e-01 -6.8136e-02  2.1104e-01  2.9287e-01 -8.8861e-02 -2.0462e-01\n",
            " -5.7602e-01  3.4526e-01  4.1390e-01  1.7917e-01  2.5143e-01 -2.2678e-01\n",
            " -1.0103e-01  1.4576e-01  2.0127e-01  3.1810e-01 -7.8907e-01 -2.2194e-01\n",
            " -2.4833e-01 -1.5103e-02 -2.0050e-01 -2.6441e-02  1.8551e-01  3.3782e-01\n",
            " -3.3543e-01  8.6117e-01 -4.7083e-02 -1.7009e-01  3.0438e-01  9.4119e-02\n",
            "  3.2435e-01 -8.1171e-01  8.8966e-01 -3.9149e-01  1.6828e-01  1.4316e-01\n",
            "  3.6339e-03 -6.4557e-02  4.5777e-02 -3.2248e-01  4.8943e-02  1.6817e-01\n",
            "  6.8344e-02  5.4227e-01  1.2493e-01  6.9742e-01 -3.7194e-02  3.3080e-01\n",
            " -4.2194e-01  3.3970e-01  2.7646e-01 -1.6003e-02 -2.1827e-01  4.4535e-01\n",
            "  3.5379e-01 -2.2089e-02  2.1375e-01  4.3267e-01 -3.2897e-01  9.6165e-02\n",
            "  3.1265e-01 -3.0528e-01  2.6126e-01 -6.5364e-01 -7.8014e-01 -2.3154e-01\n",
            "  1.2113e-01  3.4896e-01 -5.5444e-01  4.6619e-01 -1.6520e-01  1.1611e-01\n",
            " -7.6676e-01  6.9502e-01 -1.5698e-01 -1.2490e-01  5.6505e-01  6.4499e-01\n",
            " -5.7403e-01 -3.3549e-02  3.2898e-01 -1.4025e+00 -3.1143e-01  6.4549e-01\n",
            " -6.1534e-02 -6.9295e-01  6.0894e-04 -5.6544e-01  1.9181e-01 -1.9208e-01\n",
            " -6.2673e-01 -9.7473e-03 -5.5040e-01 -5.6128e-01 -1.9603e-01  2.9254e-01\n",
            "  9.8576e-02 -5.9395e-02  3.3616e-03  1.9515e-01 -6.0703e-01  3.4262e-01\n",
            "  9.5211e-02 -7.9411e-02  1.4305e-01 -5.6569e-01 -6.5887e-02  1.5167e-01\n",
            " -1.3505e-01  1.9571e-01  2.2812e-01  3.5346e-02 -2.2509e-01  1.8910e-01\n",
            " -3.7348e-01  1.2505e-01  4.6249e-01 -3.2219e-01  9.0643e-01  1.1595e-01\n",
            "  1.1628e-01  2.2961e-01  2.4010e-01 -6.1609e-02  3.9325e-01 -6.5066e-02\n",
            "  4.2257e-01  5.6880e-01  4.9804e-01 -6.1308e-01  4.1468e-01 -1.3448e-01\n",
            "  6.0430e-01 -6.5462e-02 -8.5376e-02  1.9115e-01  3.9925e-01  3.7495e-01\n",
            " -1.8492e-01  6.1751e-02 -3.8747e-01 -3.0335e-01 -3.8211e-01  2.8221e-01\n",
            " -1.0286e-01 -5.8660e-01  8.2922e-01  2.5131e-01  2.4772e-01  8.7482e-01\n",
            " -3.1359e-01  8.1621e-01 -9.0081e-01 -7.7933e-01 -1.0090e+00  3.6472e-01\n",
            " -1.1562e-01 -2.4841e-01  9.4527e-02 -4.2266e-01  6.0392e-02 -1.5365e-01\n",
            " -6.9604e-02  5.1292e-03  3.9572e-01 -1.5692e-01  3.5708e-01 -3.5165e-01\n",
            "  3.5296e-01 -5.2220e-01  5.1400e-01 -1.7764e-01 -1.0272e-01 -3.9640e-01\n",
            "  3.0418e-01  7.3659e-02 -1.1685e-01  1.4299e-01 -3.6810e-01  2.7642e-01\n",
            " -4.6683e-01 -3.2633e-01  5.1107e-01  2.3945e-02  1.1723e-01  2.1761e-01\n",
            " -1.7389e-01 -6.1193e-01 -5.9449e-01  4.7749e-01 -5.9008e-01 -3.6092e-01\n",
            " -9.9574e-02 -4.3098e-02 -1.5106e-01 -1.4336e-01 -3.1135e-02  1.7887e-01\n",
            " -6.4221e-01  1.7242e-01  3.3916e-01  8.7181e-01 -7.7230e-01  5.3195e-01\n",
            " -5.2763e-01  1.7510e-01  3.1043e-01 -1.5177e-01 -2.2706e-01  1.0803e-01\n",
            "  4.4919e-01  7.0016e-02  2.0851e-01  2.1517e-01 -6.1712e-01 -9.9970e-02\n",
            "  5.5020e-03  7.6786e-02  2.8046e-01  4.2331e-01 -5.8925e-01  7.0554e-02\n",
            "  3.9923e-01  9.0201e-02  1.7139e-01 -1.7282e-01 -5.3675e-01 -4.6439e-01\n",
            " -5.7850e-01 -6.8311e-01  5.9383e-02  1.2427e-01 -1.4558e-01  5.7687e-01\n",
            " -5.7499e-01 -5.1645e-02  3.8410e-01  1.3047e-01  3.3786e-01  3.3204e-01\n",
            "  4.0119e-01  2.6389e-01 -3.6953e-01 -2.9797e-01 -6.6816e-01 -1.1883e-01\n",
            "  5.0133e-01  2.0603e-01 -3.2558e-01 -1.2242e-01  5.0666e-01  1.6353e-01\n",
            " -1.0672e-01  2.2364e-01  2.3915e-01 -5.5509e-01 -4.8432e-01 -1.2165e-02\n",
            " -1.7992e+00  3.2310e-01 -2.6309e-01 -3.2538e-01 -5.8270e-01  1.5099e-01\n",
            "  3.3838e-01  1.2007e-01  4.1395e-01 -1.5553e-01 -1.9301e-01  5.8860e-02\n",
            " -5.2420e-01 -3.7170e-01  5.6205e-01 -6.5801e-01 -4.9796e-01  2.4347e-01\n",
            "  1.2873e-01  3.3665e-01 -7.2609e-02 -1.5686e-01 -1.4187e-01 -2.6488e-01]\n"
          ]
        }
      ],
      "source": [
        "# example of a word as a vector\n",
        "word_vectors = model\n",
        "\n",
        "# vector embedding of a word look like\n",
        "print(word_vectors['computer'])   # 300 dimensional vector of a comp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJ_-o1y4PNV5",
        "outputId": "03f475c1-40ac-459f-8f28-a4c629dc37f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(300,)\n"
          ]
        }
      ],
      "source": [
        "print(word_vectors['cat'].shape)\n",
        "# every word is encoded into 300 dimensional vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofqmMD-DQk3z"
      },
      "source": [
        "King + woman - man = ? (queen should be the ans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URlSlld_Pis2",
        "outputId": "4b5b9955-9fa0-47dd-8038-a53fd168bc8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('queen', 0.6713277101516724), ('princess', 0.5432624816894531), ('throne', 0.5386103987693787), ('monarch', 0.5347574949264526), ('daughter', 0.49802514910697937), ('mother', 0.49564430117607117), ('elizabeth', 0.4832652509212494), ('kingdom', 0.47747090458869934), ('prince', 0.4668239951133728), ('wife', 0.46473270654678345)]\n"
          ]
        }
      ],
      "source": [
        "# example of using most_similar\n",
        "print(word_vectors.most_similar(positive=['king', 'woman'], negative=['man'], topn=10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjoSgnfDRMB9",
        "outputId": "55638b99-79af-4533-830d-c7a9c2a29786"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6998663\n",
            "0.6336469\n",
            "0.6660684\n",
            "0.8272891\n",
            "0.73385984\n",
            "0.24615553\n"
          ]
        }
      ],
      "source": [
        "# checking similarity b/w a few pair of words\n",
        "# eg of calculating similarity\n",
        "print(word_vectors.similarity('woman', 'man'))\n",
        "print(word_vectors.similarity('king', 'queen'))\n",
        "print(word_vectors.similarity('uncle', 'aunt'))\n",
        "print(word_vectors.similarity('boy', 'girl'))\n",
        "print(word_vectors.similarity('nephew', 'niece'))\n",
        "print(word_vectors.similarity('paper', 'water'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uc4GKQ3kR1po",
        "outputId": "d5b24c10-7fef-4f8c-9e88-185462562194"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('towers', 0.7919201850891113), ('skyscraper', 0.6111851930618286), ('building', 0.5957720279693604), ('spire', 0.5896912813186646), ('tallest', 0.5712063312530518)]\n"
          ]
        }
      ],
      "source": [
        "# most similar words\n",
        "print(word_vectors.most_similar('tower', topn=5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96nHZiWnSLG8",
        "outputId": "c5833ff3-0d05-4224-e4e3-e2f52ad64b6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.7539396\n",
            "10.218706\n",
            "3.9361057\n"
          ]
        }
      ],
      "source": [
        "# let us see the vector similarity\n",
        "import numpy as np\n",
        "# words to compare\n",
        "word1 = 'man'\n",
        "word2 = 'woman'\n",
        "\n",
        "word3 = 'semiconductor'\n",
        "word4 = 'earthworm'\n",
        "\n",
        "word5 = 'nephew'\n",
        "word6 = 'niece'\n",
        "\n",
        "# calculate the vector diff\n",
        "vector_diff1 = model[word1] - model[word2]\n",
        "vector_diff2 = model[word3] - model[word4]\n",
        "vector_diff3 = model[word5] - model[word6]\n",
        "\n",
        "# calculate magnitude of the vector diff\n",
        "magnitude1 = np.linalg.norm(vector_diff1)\n",
        "magnitude2 = np.linalg.norm(vector_diff2)\n",
        "magnitude3 = np.linalg.norm(vector_diff3)\n",
        "\n",
        "# print magnitude of diff\n",
        "print(magnitude1)   # man & woman\n",
        "print(magnitude2)   # semiconductor & earthworm\n",
        "print(magnitude3)   # nephew & niece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNKg9YM0TFe8"
      },
      "outputs": [],
      "source": [
        "# creating token embeddings\n",
        "input_ids = torch.tensor([2,3,5,1])  # representing everything as tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjPS5IUmXSnT"
      },
      "outputs": [],
      "source": [
        "# let's take vocab size 6 and embeddings of size 3\n",
        "# 3 cols & 6 rows\n",
        "vocab_size = 6\n",
        "output_dim = 3   # every token will be converted into vector of 3 dimensions\n",
        "\n",
        "torch.manual_seed(123)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZijqp1NY1xM",
        "outputId": "34864840-e576-4b65-b356-aded9c9fa711"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3374, -0.1778, -0.1690],\n",
            "        [ 0.9178,  1.5810,  1.3010],\n",
            "        [ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-1.1589,  0.3255, -0.6315],\n",
            "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "print(embedding_layer.weight)\n",
        "# these are the initial weighs which needs to be optimized\n",
        "# embedding is also called as simple look up table as we can see from below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLsX4w87ZS3S",
        "outputId": "82c90f44-f7d2-4ff6-acfc-0d8ee0c20956"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(embedding_layer(torch.tensor([3])))   # vector for id=3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DLeDqWRbRdC",
        "outputId": "f0c87fd8-e580-403e-896c-0c497218c0a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-2.8400, -0.7849, -1.4096],\n",
            "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(embedding_layer(input_ids))\n",
        "# row no. 3,4,6,2 as above i/p_ids we've specified"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX649s6dJmo6"
      },
      "source": [
        "Positional embeddings (encoding word positions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ki7RU3QDcMkS"
      },
      "outputs": [],
      "source": [
        "vocab_size = 50257  # no. of rows\n",
        "output_dim = 256    # smaller than GPT-3  # no. of cols\n",
        "\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ct8FKcG-Kuyy"
      },
      "outputs": [],
      "source": [
        "max_length = 4     # 4 i/p tokens will be used to predict the next word\n",
        "dataloader = create_data_loader_v1(\n",
        "    raw_text, tokenizer, batch_size=8, max_length=max_length,\n",
        "    stride=max_length, shuffle=False)\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLmG_Z_RLj_M",
        "outputId": "24a153f1-0c9c-42dd-af45-c8e97a213c87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token ids:\n",
            " [tensor([   40,  1807, 10899, 15632,   922,   568,  1049,   284]), tensor([ 367, 3619, 2138,  438, 5891,  340, 5975, 3285]), tensor([2885,  402,  257, 2016, 1576,  373,  284,  326]), tensor([1464,  271, 7026,  257,  438,  645,  502,   11])]\n",
            "\n",
            "input shape:\n",
            " [8, 8, 8, 8]\n"
          ]
        }
      ],
      "source": [
        "print(\"token ids:\\n\", inputs)\n",
        "print(\"\\ninput shape:\\n\", [len(row) for row in inputs])  # Get the length of each row in the list\n",
        "# 8x4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cs_mvWk2MBNN",
        "outputId": "ab83bf9e-a24a-43e9-9e01-1f17b583b188"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 8, 256])\n"
          ]
        }
      ],
      "source": [
        "# Convert the list of lists (inputs) into a 2D tensor of type long\n",
        "input_tensor = torch.stack(inputs, dim=0).type(torch.long)  # Use torch.stack to combine tensors\n",
        "\n",
        "# Now pass the correct tensor to the embedding layer\n",
        "token_embeddings = embedding_layer(input_tensor)\n",
        "print(token_embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apJsKJ1XREwq"
      },
      "outputs": [],
      "source": [
        "context_length = max_length\n",
        "pos_embeddings_layer = torch.nn.Embedding(context_length, output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhEpHUndSLOV",
        "outputId": "d4a9baad-8fe8-4575-dec3-dee625e4091f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ],
      "source": [
        "pos_embeddings = pos_embeddings_layer(torch.arange(max_length))  # 0, 1, ... upto max i/p length -1\n",
        "print(pos_embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrLSLOJySamN",
        "outputId": "bbf4f2ce-de11-4fd2-ae6d-ac53de93a943"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 8, 256])\n"
          ]
        }
      ],
      "source": [
        "# Assuming token_embeddings has shape (batch_size, sequence_length, embedding_dim) which is (8, 4, 256)\n",
        "# and pos_embeddings has shape (sequence_length, embedding_dim) which is (4, 256)\n",
        "# We need to expand pos_embeddings to have shape (1, sequence_length, embedding_dim) which is (1, 4, 256)\n",
        "# before broadcasting it to the shape of token_embeddings\n",
        "\n",
        "input_embeddings = token_embeddings + pos_embeddings[:, None, :].expand(token_embeddings.shape)\n",
        "# expand pos_embeddings to have the same batch size as token_embeddings\n",
        "\n",
        "print(input_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQNAUxeNqmcD"
      },
      "source": [
        "Implemented a simplified attention mechanism"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqh8zZJ_p2fk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "inputs = torch.tensor(\n",
        "    [[0.43, 0.15, 0.89],  # your\n",
        "     [0.55, 0.87, 0.66],  # journey\n",
        "     [0.57, 0.85, 0.64],  # starts\n",
        "     [0.22, 0.58, 0.33],  # with\n",
        "     [0.77, 0.25, 0.10],  # one\n",
        "     [0.05, 0.80, 0.55]]  # step\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOefXFlNs75T"
      },
      "source": [
        "we can use dot product to find the similarity between between the two vectors as if 0 angle will be there then then cos 0 is 1 means most similar (higher the dot product more align the vectors are, lower means not align)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GB_O7eFvs68X",
        "outputId": "661ef00f-b51b-40d6-99bf-c0def3310ca8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
          ]
        }
      ],
      "source": [
        "query = inputs[1]  # 0 based indexing, 2nd i/p token is the query\n",
        "\n",
        "attn_scores_2 = torch.empty(inputs.shape[0])\n",
        "for i,x_i in enumerate(inputs):\n",
        "    attn_scores_2[i] = torch.dot(x_i, query) # dot product b/w every i/p and query vector\n",
        "\n",
        "print(attn_scores_2)\n",
        "# 2nd, 3rd and 6th value have the largest attention score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "024fS3x0vubc"
      },
      "source": [
        "normalize scores that we computed previosly\n",
        "\n",
        "main goal of normalization is to obtain attention weights that sum upto 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvAfY3hOrggP",
        "outputId": "2c4b17ae-3da5-42cb-90ae-0be7704f32cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
            "sum: tensor(1.0000)\n"
          ]
        }
      ],
      "source": [
        "attn_weights_2_tmp = attn_scores_2/attn_scores_2.sum()\n",
        "\n",
        "print(\"Attention weights:\", attn_weights_2_tmp)\n",
        "print(\"sum:\", attn_weights_2_tmp.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMo50KmDqctQ",
        "outputId": "baa02692-c551-45db-f359-7fb9bb2076bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
            "sum: tensor(1.)\n"
          ]
        }
      ],
      "source": [
        "def softmax(x):\n",
        "    return torch.exp(x)/torch.exp(x).sum(dim=0)\n",
        "\n",
        "attn_weights_2_naive = softmax(attn_scores_2)\n",
        "\n",
        "print(\"attention weights:\", attn_weights_2_naive)\n",
        "print(\"sum:\", attn_weights_2_naive.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXTrQPptr0Ja",
        "outputId": "6e7664c4-a447-441e-e78a-aa831b927983"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4419, 0.6515, 0.5683])\n"
          ]
        }
      ],
      "source": [
        "query = inputs[1] # 2nd i/p token is the query\n",
        "\n",
        "context_vec_2 = torch.zeros(query.shape)\n",
        "for i, x_i in enumerate(inputs):\n",
        "    context_vec_2 += attn_weights_2_naive[i] * x_i\n",
        "\n",
        "print(context_vec_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlmXB_jouCLb",
        "outputId": "91d5dfac-f771-430e-faf6-9a4747375c9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
          ]
        }
      ],
      "source": [
        "attn_scores = torch.empty(6,6)\n",
        "for i, x_i in enumerate(inputs):\n",
        "    for j, x_j in enumerate(inputs):\n",
        "        attn_scores[i,j] = torch.dot(x_i, x_j)\n",
        "\n",
        "print(attn_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_epu1tghu5xS",
        "outputId": "80d3f516-ccd2-4d9f-80cc-57aa3902a268"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
          ]
        }
      ],
      "source": [
        "# this one is same as upper one\n",
        "attn_scores = inputs @ inputs.T      # i/p and its transpose\n",
        "print(attn_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJHfifdrwEW9",
        "outputId": "bd393005-c654-4c23-c9a1-0a272eeb02bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
            "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
            "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
            "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
            "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
            "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
          ]
        }
      ],
      "source": [
        "# normalization\n",
        "attn_weights = torch.softmax(attn_scores, dim=-1)  # by setting dim=-1 we're normalizing the attn_scores tensor\n",
        "print(attn_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqSwr6rywWB7",
        "outputId": "242c8d81-bd66-49cf-cb55-0ad42af91f7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "row 2 sum: 1.0\n",
            "all row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
          ]
        }
      ],
      "source": [
        "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
        "print(\"row 2 sum:\", row_2_sum)\n",
        "print(\"all row sums:\", attn_weights.sum(dim=-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upgoE5RBxHeM",
        "outputId": "2d5e3251-eabc-4d85-e46e-5b5aae1e1b95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4421, 0.5931, 0.5790],\n",
            "        [0.4419, 0.6515, 0.5683],\n",
            "        [0.4431, 0.6496, 0.5671],\n",
            "        [0.4304, 0.6298, 0.5510],\n",
            "        [0.4671, 0.5910, 0.5266],\n",
            "        [0.4177, 0.6503, 0.5645]])\n"
          ]
        }
      ],
      "source": [
        "all_context_vecs = attn_weights @ inputs\n",
        "print(all_context_vecs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ippeje5Xxlzf",
        "outputId": "f9181be2-5595-4f06-8633-09b3b80765ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "previous 2nd context vector: tensor([0.4419, 0.6515, 0.5683])\n"
          ]
        }
      ],
      "source": [
        "print(\"previous 2nd context vector:\",context_vec_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wZ3p-BUheLx"
      },
      "source": [
        "Query key and the value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9tvZeL4yoov"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "inputs = torch.tensor(\n",
        "    [[0.43, 0.15, 0.89],  # your\n",
        "     [0.55, 0.87, 0.66],  # journey\n",
        "     [0.57, 0.85, 0.64],  # starts\n",
        "     [0.22, 0.58, 0.33],  # with\n",
        "     [0.77, 0.25, 0.10],  # one\n",
        "     [0.05, 0.80, 0.55]]  # step\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8cbmi33h5QK"
      },
      "outputs": [],
      "source": [
        "x_2 = inputs[1]  #A  #corresponds to journey\n",
        "d_in = inputs.shape[1]  #B\n",
        "d_out = 2  #C\n",
        "\n",
        "# GPT-like models have same i/p o/p dimensions usually\n",
        "# but for illustration purpose we choose d_in=3, d_out=2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZE8UIN_shjXs"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(123)\n",
        "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbXGViBXiTJS",
        "outputId": "88f2487e-02c9-43be-9420-2b5a8b291774"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[0.2961, 0.5166],\n",
            "        [0.2517, 0.6886],\n",
            "        [0.0740, 0.8665]])\n"
          ]
        }
      ],
      "source": [
        "print(W_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mlPSh9-iqqs",
        "outputId": "b42e8caf-52d1-4054-c88e-742336cd63c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[0.1366, 0.1025],\n",
            "        [0.1841, 0.7264],\n",
            "        [0.3153, 0.6871]])\n"
          ]
        }
      ],
      "source": [
        "print(W_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwY1F_DZixdw",
        "outputId": "fdfc2df7-65bc-4900-8a51-9e4d4d01057a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[0.0756, 0.1966],\n",
            "        [0.3164, 0.4017],\n",
            "        [0.1186, 0.8274]])\n"
          ]
        }
      ],
      "source": [
        "print(W_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1muyeGzi0Xt",
        "outputId": "b9018edc-79f1-40db-bd40-bded152efae5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4306, 1.4551])\n"
          ]
        }
      ],
      "source": [
        "query_2 = x_2 @ W_query\n",
        "key_2 = x_2 @ W_key\n",
        "value_2 = x_2 @ W_value\n",
        "print(query_2)   # for journey"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34SSxjiDjurx",
        "outputId": "83ec6278-e036-4567-f6f4-9307b1b795f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keys.shape: torch.Size([6, 2])\n",
            "queries.shape: torch.Size([6, 2])\n",
            "values.shape: torch.Size([6, 2])\n"
          ]
        }
      ],
      "source": [
        "keys = inputs @ W_key\n",
        "queries = inputs @ W_query\n",
        "values = inputs @ W_value\n",
        "print(\"keys.shape:\", keys.shape)\n",
        "print(\"queries.shape:\", queries.shape)\n",
        "print(\"values.shape:\", values.shape)\n",
        "# 6 i/p tokens and for each i/p vector we've a 2D key vector, 2D query and value vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ied44MdFkEbu",
        "outputId": "c1e97b9b-6743-4a4f-d98d-5d605d23e3b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.8524)\n"
          ]
        }
      ],
      "source": [
        "keys_2 = keys[1]\n",
        "attn_score_22 = query_2.dot(keys_2)\n",
        "print(attn_score_22)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nhjZhFamDCN",
        "outputId": "aedad217-14d8-4b4c-9459-a6858845e0c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
          ]
        }
      ],
      "source": [
        "attn_scores_2 = query_2 @ keys.T   # dot product between 2nd query and keys metrics\n",
        "print(attn_scores_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwidNKEQpSgl",
        "outputId": "815a9746-994d-43de-f560-c2bebe1ad63d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9231, 1.3545, 1.3241, 0.7910, 0.4032, 1.1330],\n",
            "        [1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
            "        [1.2544, 1.8284, 1.7877, 1.0654, 0.5508, 1.5238],\n",
            "        [0.6973, 1.0167, 0.9941, 0.5925, 0.3061, 0.8475],\n",
            "        [0.6114, 0.8819, 0.8626, 0.5121, 0.2707, 0.7307],\n",
            "        [0.8995, 1.3165, 1.2871, 0.7682, 0.3937, 1.0996]])\n"
          ]
        }
      ],
      "source": [
        "attn_scores = queries @ keys.T\n",
        "print(attn_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StL4wm4Hp00t",
        "outputId": "14d2bcf0-6fce-40bf-dd86-98b6ad6943a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "# attention weights (by dividing them by square root of embedding dimen of keys)\n",
        "d_k = keys.shape[-1]   # -1 cuz lokking at the col\n",
        "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
        "print(attn_weights_2)\n",
        "print(d_k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imdhN7wCZhHA",
        "outputId": "b946a5f4-ce38-4298-8e4c-0855653b3a80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "softmax without scaling: <function softmax at 0x7ec75fbe67a0>\n",
            "softmax with scaling: tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# define the tensor\n",
        "tensor = torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])\n",
        "\n",
        "# apply softmax without scaling\n",
        "softmax_result = torch.softmax(tensor, dim=-1)\n",
        "print(\"softmax without scaling:\", softmax)\n",
        "\n",
        "# multiply the tensor by 8 then apply softmax\n",
        "scaled_tensor = tensor*8;\n",
        "softmax_scaled_result = torch.softmax(scaled_tensor, dim=-1)\n",
        "print(\"softmax with scaling:\", softmax_scaled_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ii1sr4gFao22"
      },
      "outputs": [],
      "source": [
        "# why sqrt\n",
        "# to make variance of the dot product stable\n",
        "# dot product of Q and K increases variance because multiplying two random nums increases the variance\n",
        "# increase in variance grows with the dimension\n",
        "# dividing by sqrt (dimension) keeps variance close to 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmoKBJfNbARp",
        "outputId": "e3f34c7b-0a7b-4eee-ad5e-6aa27b7922a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "variance before scaling (dim=5): 4.687361557162383\n",
            "Variance before scaling (dim=5): 0.9374723114324767\n",
            "variance before scaling (dim=20): 20.464034999870393\n",
            "Variance before scaling (dim=20): 1.0232017499935198\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_variance(dim, num_trials=1000):\n",
        "    dot_products = []\n",
        "    scaled_dot_products = []\n",
        "\n",
        "    for _ in range(num_trials):\n",
        "        # Generate two random vectors with the specified dimension\n",
        "        q = np.random.randn(dim)\n",
        "        k = np.random.randn(dim)\n",
        "\n",
        "        dot_product = np.dot(q, k)\n",
        "        dot_products.append(dot_product)\n",
        "\n",
        "        scaled_dot_product = dot_product / np.sqrt(dim)   # sqrt makes sure that after scaling variance keeps close to 1\n",
        "        scaled_dot_products.append(scaled_dot_product)\n",
        "\n",
        "    variance = np.var(dot_products)\n",
        "    scaled_variance = np.var(scaled_dot_products)\n",
        "\n",
        "    return variance, scaled_variance\n",
        "\n",
        "# for dimension 5\n",
        "variance_before_5, variance_after_5 = compute_variance(5)\n",
        "print(f\"variance before scaling (dim=5): {variance_before_5}\")\n",
        "print(f\"Variance before scaling (dim=5): {variance_after_5}\")\n",
        "\n",
        "# for dimension 20\n",
        "variance_before_20, variance_after_20 = compute_variance(20)\n",
        "print(f\"variance before scaling (dim=20): {variance_before_20}\")\n",
        "print(f\"Variance before scaling (dim=20): {variance_after_20}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFe8l0dSdXIr",
        "outputId": "a5df6bdf-079b-4b34-899d-c83fb8b047e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.3061, 0.8210])\n"
          ]
        }
      ],
      "source": [
        "# for single context vectors\n",
        "context_vec_2 = attn_weights_2 @ values\n",
        "print(context_vec_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9m6ZgTTwf3Tg"
      },
      "outputs": [],
      "source": [
        "# implementing a compact self attention python class  // to compute all the context vectors\n",
        "import torch.nn as nn\n",
        "\n",
        "class selfattention_v1(nn.Module):\n",
        "    def __init__(self, d_in, d_out):\n",
        "        super().__init__()\n",
        "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
        "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
        "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
        "\n",
        "    def forward(self, x):\n",
        "        keys = x @ self.W_key\n",
        "        queries = x @ self.W_query\n",
        "        values = x @ self.W_value\n",
        "\n",
        "        attn_scores = queries @ keys.T\n",
        "        attn_weights = torch.softmax(attn_scores/ keys.shape[-1]**0.5, dim=-1)   #**0.5 sqrt h\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZYqCBHN57ph",
        "outputId": "522908d0-662e-4857-e13c-44171f838ba8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2996, 0.8053],\n",
            "        [0.3061, 0.8210],\n",
            "        [0.3058, 0.8203],\n",
            "        [0.2948, 0.7939],\n",
            "        [0.2927, 0.7891],\n",
            "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "sa_v1 = selfattention_v1(d_in, d_out)\n",
        "print(sa_v1(inputs))\n",
        "# each row corresponds to context token, 1st row for 1st token and likewise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZcYpM7b7EYS"
      },
      "outputs": [],
      "source": [
        "# improving selfattention_v1 further by utilizing pytorch's nn.linear layers as it has optimized weight initialization scheme\n",
        "import torch.nn as nn\n",
        "\n",
        "class selfattention_v2(nn.Module):\n",
        "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        attn_scores = queries @ keys.T\n",
        "        attn_weights = torch.softmax(attn_scores/ keys.shape[-1]**0.5, dim=-1)   #**0.5 sqrt h\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZPHriYH9a_0",
        "outputId": "b9928cd4-2233-44ca-afb3-9c4f38d7c5fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0739,  0.0713],\n",
            "        [-0.0748,  0.0703],\n",
            "        [-0.0749,  0.0702],\n",
            "        [-0.0760,  0.0685],\n",
            "        [-0.0763,  0.0679],\n",
            "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(789)\n",
        "sa_v2 = selfattention_v2(d_in, d_out)\n",
        "print(sa_v2(inputs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaERRUEzDQ5f"
      },
      "source": [
        "Hiding future words with causal atteniton\n",
        "\n",
        "=> main purpose of causal attention is to not have any influence of future token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_8_ZFPBD77N"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "inputs = torch.tensor(\n",
        "    [[0.43, 0.15, 0.89],  # your\n",
        "     [0.55, 0.87, 0.66],  # journey\n",
        "     [0.57, 0.85, 0.64],  # starts\n",
        "     [0.22, 0.58, 0.33],  # with\n",
        "     [0.77, 0.25, 0.10],  # one\n",
        "     [0.05, 0.80, 0.55]]  # step\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXgpeb99-soX",
        "outputId": "ac281bb9-9835-4904-b614-1a73f2f6de7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
            "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
            "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
            "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
            "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "queries = sa_v2.W_query(inputs)\n",
        "keys = sa_v2.W_key(inputs)\n",
        "attn_scores = queries @ keys.T\n",
        "attn_weights = torch.softmax(attn_scores/ keys.shape[-1]**0.5, dim=-1)\n",
        "print(attn_weights)\n",
        "# here entries are already influenced by the other entries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rnjn3T4CDsVI",
        "outputId": "d20bf14f-e38e-4b1c-e859-140a0f363ce6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1.]])\n"
          ]
        }
      ],
      "source": [
        "# generate a mask\n",
        "# triu => upper triangular, lower all 0\n",
        "# tril => lower triangular, upper all 0\n",
        "context_length = attn_scores.shape[0]\n",
        "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
        "print(mask_simple)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKhx-zdXEQcW",
        "outputId": "ae0f2399-eb00-4d4c-e95d-7ea20a7faed2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
            "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "masked_simple = attn_weights * mask_simple\n",
        "print(masked_simple)\n",
        "# every row sum upto 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ik1mlxZxFCxt",
        "outputId": "78d979d2-a3e4-4599-a114-de1ece6dc7a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
            "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<DivBackward0>)\n"
          ]
        }
      ],
      "source": [
        "row_sums = masked_simple.sum(dim=1, keepdim=True)\n",
        "masked_simple_norm = masked_simple/row_sums\n",
        "print(masked_simple_norm)\n",
        "# above we've applied softmax and here again we're dividing with the row_sum which leads to data leakage problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7M0zRJjFtyz",
        "outputId": "e8b9366b-0920-486c-dafe-37eb893644d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
            "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
            "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
            "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# cancelling the influence of future tokens by introducing -ve infinity => no influence of future tokens now\n",
        "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
        "print(masked)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LlY7QliHlVc",
        "outputId": "eb6976c7-32c5-43dd-a5e2-a14d58298d44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
            "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)\n",
        "print(attn_weights)\n",
        "# everything is satisfied and rows also sunm upto one"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSkkpzJaJNg0"
      },
      "source": [
        "masking additional attention weights with dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtOoMxI9Jnre",
        "outputId": "ea926f48-5cc5-4506-b804-a6c30d8902ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1.]])\n"
          ]
        }
      ],
      "source": [
        "example = torch.ones(6,6)\n",
        "print(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txql96xCILeF",
        "outputId": "5c2b2b55-dbde-4288-b638-1a5b49dce4af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2., 2., 2., 2., 2., 2.],\n",
            "        [0., 2., 0., 0., 0., 0.],\n",
            "        [0., 0., 2., 0., 2., 0.],\n",
            "        [2., 2., 0., 0., 0., 2.],\n",
            "        [2., 0., 0., 0., 0., 2.],\n",
            "        [0., 2., 0., 0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "# we'll use a dropout rate of 50%, means masking out half of the attention weights\n",
        "torch.manual_seed(123)\n",
        "dropout = torch.nn.Dropout(0.5)    # on an avg it will swiitch of 50% of the weights, and rescale all the other weights by that much amt\n",
        "example = torch.ones(6,6)\n",
        "print(dropout(example))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCSyQN8HJ4qV",
        "outputId": "cea2a6ca-8232-4d00-efb2-b3c13a10f34b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.8966, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.6206, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5517, 0.4921, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4350, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.3327, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "print(dropout(attn_weights))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Py8OH2yxKpzF",
        "outputId": "c6ead4ec-56a9-4691-b25c-50bc4fc89469"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 6, 3])\n"
          ]
        }
      ],
      "source": [
        "batch = torch.stack((inputs,inputs),dim=0)\n",
        "print(batch.shape)\n",
        "# batch_size, no. of token, vector embedding dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huBy_l4pLZmn"
      },
      "outputs": [],
      "source": [
        "class causalattention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.d_out = d_out\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('mask',torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        attn_scores = queries @ keys.transpose(1,2)\n",
        "        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
        "        attn_weights = torch.softmax(attn_scores/ keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec\n",
        "\n",
        "# this makes all the upper part zero and all the rows sum upto 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVAsyceTNhz1",
        "outputId": "721a9615-ce86-4675-a257-b484cff1aea9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "context_vecs.shape: torch.Size([2, 6, 2])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "context_length = batch.shape[1]\n",
        "ca = causalattention(d_in, d_out, context_length, 0.0)   # 0.0 is the dropout\n",
        "context_vec = ca(batch)\n",
        "print(\"context_vecs.shape:\",context_vec.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9AhTEARN1Ux",
        "outputId": "3c632445-c60c-4202-ecef-cf9b4c094cdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.4519,  0.2216],\n",
            "         [-0.5874,  0.0058],\n",
            "         [-0.6300, -0.0632],\n",
            "         [-0.5675, -0.0843],\n",
            "         [-0.5526, -0.0981],\n",
            "         [-0.5299, -0.1081]],\n",
            "\n",
            "        [[-0.4519,  0.2216],\n",
            "         [-0.5874,  0.0058],\n",
            "         [-0.6300, -0.0632],\n",
            "         [-0.5675, -0.0843],\n",
            "         [-0.5526, -0.0981],\n",
            "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(context_vec)\n",
        "# 1st i/p\n",
        "# 2nd i/p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxLEKGAC2Yuz"
      },
      "source": [
        "Extending single head attention to multi-head attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIpKwMsk2YY3"
      },
      "outputs": [],
      "source": [
        "# we can achieve this by implementing a simple multihead attention wrapper class that stacks multiple instances of our previously implemented causal attention module\n",
        "class MultiHeadAttentionWrapper (nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(\n",
        "            [causalattention(d_in, d_out, context_length, dropout, qkv_bias)\n",
        "            for _ in range(num_heads)]\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "\n",
        "# if we use multi with two attention head (num_head=2) & causal output dim with two (d_out*num_head=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFf3sV723qKO",
        "outputId": "1b24b541-04b3-4ffe-cf34-049d8dee1f70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 6, 3])\n"
          ]
        }
      ],
      "source": [
        "inputs = torch.tensor(\n",
        "    [[0.43, 0.15, 0.89],  # your\n",
        "     [0.55, 0.87, 0.66],  # journey\n",
        "     [0.57, 0.85, 0.64],  # starts\n",
        "     [0.22, 0.58, 0.33],  # with\n",
        "     [0.77, 0.25, 0.10],  # one\n",
        "     [0.05, 0.80, 0.55]]  # step\n",
        ")\n",
        "\n",
        "batch = torch.stack((inputs,inputs),dim=0)\n",
        "print(batch.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTqPxIv95L02",
        "outputId": "39487bcb-5ed5-4b26-df2e-e415ad9f2e26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
            "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
            "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
            "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
            "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
            "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
            "\n",
            "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
            "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
            "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
            "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
            "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
            "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 6, 4])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "context_length = batch.shape[1]\n",
        "d_in, d_out = 3,2\n",
        "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
        "context_vecs = mha(batch)\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)\n",
        "\n",
        "# d_out is 2 but 4 colms qki two attention head h (num_heads=2) so two two are aggregated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ffqs2F0hrqb"
      },
      "source": [
        "Implementing multi head attention with weight splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jfrgXpVhVKF"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert d_out % num_heads == 0, \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            'mask',\n",
        "            torch.triu(torch.ones(context_length, context_length),\n",
        "                       diagonal=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x)  # shape: (b_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # 3d is converted to 4d\n",
        "        # unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # (b, num_tokens, num_head, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        # grouping w.r.t no. of heads\n",
        "        keys = keys.transpose(1,2)   # the index we need to transpose is 1, 2\n",
        "        queries = queries.transpose(1,2)\n",
        "        values = values.transpose(1,2)\n",
        "\n",
        "        attn_scores = queries @ keys.transpose(2,3)\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores/ keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        context_vec = (attn_weights @ values).transpose(1,2)\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec)\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_2NOmL4upmT",
        "outputId": "3999ea4e-d869-41e1-89aa-7b9d323fe23d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 6])\n",
            "tensor([[[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
            "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
            "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]],\n",
            "\n",
            "        [[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
            "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
            "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 3, 6])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "inputs = torch.tensor(\n",
        "    [[0.43,0.15,0.89,0.55,0.87,0.66],  #row 1\n",
        "     [0.57,0.85,0.64,0.22,0.58,0.33],  #row 2\n",
        "     [0.77,0.25,0.10,0.05,0.80,0.55]]  #row 3\n",
        ")\n",
        "\n",
        "batch = torch.stack((inputs,inputs),dim=0)\n",
        "print(batch.shape)\n",
        "\n",
        "batch_size, context_length, d_in = batch.shape\n",
        "d_out = 6\n",
        "mha = MultiHeadAttention(d_in,d_out,context_length,0.0,num_heads=2)\n",
        "context_vecs = mha(batch)\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWxAwpmBhbQS"
      },
      "source": [
        "Implementing a GPT model from scratch to generate text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZAjuaFBhvku"
      },
      "outputs": [],
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,      # vocabulary size\n",
        "    \"context_length\": 1024,   # context length\n",
        "    \"emb_dim\": 768,           # embedding dimension\n",
        "    \"n_heads\": 12,            # no. of attention heads\n",
        "    \"n_layers\": 12,           # no. of layers\n",
        "    \"drop_rate\": 0.1,         # dropout rate\n",
        "    \"qkv_bias\": False         # query-key-value bias\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVT7hWseissj"
      },
      "source": [
        "GPT Architecture part 1: Dummy GPT model class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSAej5AhioMu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn   # for embedding\n",
        "\n",
        "class DummyGPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"],cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        # placeholder for transformer\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
        "        )\n",
        "\n",
        "        # placeholder for layer norm\n",
        "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)     # transformer block\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits\n",
        "\n",
        "class DummyTransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "       super().__init__()\n",
        "\n",
        "    def forward(Self,x):\n",
        "        return x\n",
        "\n",
        "class DummyLayerNorm(nn.Module):\n",
        "    def __init__(self, normalized_shape, eps=1e-5):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "124z_ELQjXf5",
        "outputId": "89bf3b03-d5da-41f9-e507-79306c098795"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n"
          ]
        }
      ],
      "source": [
        "# step 1: tokenization\n",
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "batch = []\n",
        "txt1 = \"Every effort moves you\"\n",
        "txt2 = \"Every day holds a\"\n",
        "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
        "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
        "batch = torch.stack(batch, dim=0)\n",
        "print(batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJr1b0lPqzJT",
        "outputId": "c04c5348-ef6d-4397-d77a-e88a8a96f36a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OUTPUT SHAPE torch.Size([2, 4, 50257])\n",
            "tensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],\n",
            "         [-0.4476,  0.1726,  0.5354,  ..., -0.3932,  1.5285,  0.8557],\n",
            "         [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],\n",
            "         [ 0.0447,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],\n",
            "\n",
            "        [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],\n",
            "         [-0.8422,  0.8243, -0.1098,  ..., -0.1434,  0.2079,  1.2046],\n",
            "         [ 0.1355,  1.1858, -0.1453,  ...,  0.0869, -0.1590,  0.1552],\n",
            "         [ 0.1666, -0.8138,  0.2307,  ...,  2.5035, -0.3055, -0.3083]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# step 2: create an instance of dummyGPTModel\n",
        "torch.manual_seed(123)\n",
        "model = DummyGPTModel(GPT_CONFIG_124M)\n",
        "logits = model(batch)\n",
        "print(\"OUTPUT SHAPE\",logits.shape)\n",
        "print(logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmsPDO-jwJYf"
      },
      "source": [
        "GPT Architecture Part 2: layer normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoLfBwvywJDZ",
        "outputId": "3ede7fbc-2c26-470a-fbd9-eabbd2dcbeb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
            "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
            "       grad_fn=<ReluBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "batch_example = torch.randn(2,5)\n",
        "layer = nn.Sequential(nn.Linear(5,6),nn.ReLU())\n",
        "out = layer(batch_example)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHsBB1_pxMdv",
        "outputId": "a534288b-2706-464c-8479-86b94cf51813"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean: tensor([[0.1324],\n",
            "        [0.2170]], grad_fn=<MeanBackward1>)\n",
            "variance: tensor([[0.0231],\n",
            "        [0.0398]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ],
      "source": [
        "mean = out.mean(dim=-1, keepdim=True)\n",
        "var = out.var(dim=-1, keepdim=True)\n",
        "print(\"mean:\",mean)\n",
        "print(\"variance:\",var)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QX6SbRIsxpV7",
        "outputId": "dada81b1-2707-4cde-b299-cc50dcaf4df4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "normalized layer outputs:\n",
            " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
            "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
            "       grad_fn=<DivBackward0>)\n",
            "mean: tensor([[9.9341e-09],\n",
            "        [1.9868e-08]], grad_fn=<MeanBackward1>)\n",
            "variance: tensor([[1.0000],\n",
            "        [1.0000]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ],
      "source": [
        "out_norm = (out - mean)/torch.sqrt(var)    #normalized\n",
        "mean = out_norm.mean(dim=-1, keepdim=True)\n",
        "var = out_norm.var(dim=-1, keepdim=True)\n",
        "print(\"normalized layer outputs:\\n\",out_norm)\n",
        "print(\"mean:\",mean)\n",
        "print(\"variance:\",var)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1UeKbJHyKHa",
        "outputId": "782dd38b-bd39-4d42-d410-66e7007ab9bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean:  tensor([[    0.0000],\n",
            "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
            "variance:  tensor([[1.0000],\n",
            "        [1.0000]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.set_printoptions(sci_mode=False)\n",
        "print(\"mean: \", mean)\n",
        "print(\"variance: \", var)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icr1kSoMzX0e"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps = 1e-5\n",
        "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(dim=-1, keepdim=True)\n",
        "    var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "    norm_x = (x - mean) / torch.sqrt(var + self.eps)   # epsilion is to prevent division by zero during normalization\n",
        "    return self.scale * norm_x + self.shift"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irpAt8Nm2LeG",
        "outputId": "ea4dd0ab-272c-4f74-8fb6-fa5f8f0566a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.1115,  0.1204, -0.3696, -0.2404, -1.1969],\n",
            "        [ 0.2093, -0.9724, -0.7550,  0.3239, -0.1085]])\n"
          ]
        }
      ],
      "source": [
        "print(batch_example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dy0GB8Zc0H6U",
        "outputId": "4f09d0d0-b4d0-4d81-a317-01f9c9ff54ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean: tensor([[    -0.0000],\n",
            "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
            "variance: tensor([[1.0000],\n",
            "        [1.0000]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ],
      "source": [
        "ln = LayerNorm(emb_dim=5)\n",
        "out_ln = ln(batch_example)\n",
        "mean = out_ln.mean(dim=-1, keepdim=True)\n",
        "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
        "print(\"mean:\",mean)\n",
        "print(\"variance:\",var)\n",
        "# normalizes such as mean as 0 and variance as 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3TItJey3ERU"
      },
      "source": [
        "GPT Architecture part 3: Feedforward neural network with GELU Activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Z2eusel2BLy"
      },
      "outputs": [],
      "source": [
        "# GELU implementation function approximation used by GPT-2\n",
        "\n",
        "class GELU(nn.Module):\n",
        "  def __init__(self):\n",
        "      super().__init__()\n",
        "\n",
        "  def forward(self,x):\n",
        "      return 0.5 * x * (1 + torch.tanh(\n",
        "          torch.sqrt(torch.tensor(2.0/torch.pi)) *\n",
        "           (x + 0.044715 * torch.pow(x, 3))\n",
        "      ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFrx08hu7pyL"
      },
      "outputs": [],
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,      # vocabulary size\n",
        "    \"context_length\": 1024,   # context length\n",
        "    \"emb_dim\": 768,           # embedding dimension\n",
        "    \"n_heads\": 12,            # no. of attention heads\n",
        "    \"n_layers\": 12,           # no. of layers\n",
        "    \"drop_rate\": 0.1,         # dropout rate\n",
        "    \"qkv_bias\": False         # query-key-value bias\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTZ0hrFh7BF-"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),  # expansion\n",
        "            GELU(),                     # activation\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"])   # contraction\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yzX-HO47Sz_",
        "outputId": "0b2d8d4f-a75c-4188-e711-b4a4f9690816"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "768\n"
          ]
        }
      ],
      "source": [
        "print(GPT_CONFIG_124M[\"emb_dim\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJLyfE8e7X5Z",
        "outputId": "0cf97d6a-42c1-4213-feed-56b640343886"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 768])\n"
          ]
        }
      ],
      "source": [
        "ffn = FeedForward(GPT_CONFIG_124M)\n",
        "x = torch.rand(2, 3, 768)\n",
        "out = ffn(x)\n",
        "print(out.shape)\n",
        "# x will have 2 batches, each batch will have 3 tokens and embeddings dim of each token is going to be 768"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2xoMV94N54_"
      },
      "source": [
        "GPT Architecture part 4: Shortcut connections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZfoahoN8V3z"
      },
      "outputs": [],
      "source": [
        "class ExampleDeepNetwork1(nn.Module):\n",
        "    def __init__(self, layer_sizes, use_shortcut):\n",
        "        super().__init__()\n",
        "        self.use_shortcut = use_shortcut\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
        "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
        "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
        "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
        "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU()),\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            layer_out = layer(x)\n",
        "            if self.use_shortcut and x.shape == layer_out.shape:\n",
        "                x = x + layer_out\n",
        "            else:\n",
        "                x = layer_out\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duM6YXp_O59w"
      },
      "outputs": [],
      "source": [
        "# initialize a neural network without shortcut connection\n",
        "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
        "sample_input = torch.tensor([[1., 0., -1.]])\n",
        "torch.manual_seed(123)\n",
        "model_without_shortcut = ExampleDeepNetwork1(layer_sizes, use_shortcut=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLk5_KEIps7t",
        "outputId": "ca73376b-d565-4fd6-df5b-9dcebdc10943"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layers.0.0.weight has gradient mean of 0.00020173584925942123\n",
            "layers.1.0.weight has gradient mean of 0.00012011159560643137\n",
            "layers.2.0.weight has gradient mean of 0.0007152040489017963\n",
            "layers.3.0.weight has gradient mean of 0.0013988736318424344\n",
            "layers.4.0.weight has gradient mean of 0.005049645435065031\n"
          ]
        }
      ],
      "source": [
        "print_gradients(model_without_shortcut, sample_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7cwaDcvl0Gd"
      },
      "outputs": [],
      "source": [
        "def print_gradients(model,x):\n",
        "    # forward pass\n",
        "    output = model(x)\n",
        "    target = torch.tensor([[0.]])\n",
        "\n",
        "    # calculate loss based on how close the target & o/p are\n",
        "    loss = nn.MSELoss()\n",
        "    loss = loss(output,target)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5x80XZESowg9",
        "outputId": "a06c6b2a-73e9-42e9-b580-abb83d3fee6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layers.0.0.weight has gradient mean of 0.22169791162014008\n",
            "layers.1.0.weight has gradient mean of 0.20694105327129364\n",
            "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
            "layers.3.0.weight has gradient mean of 0.2665732204914093\n",
            "layers.4.0.weight has gradient mean of 1.3258540630340576\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "model_with_shortcut = ExampleDeepNetwork1(layer_sizes, use_shortcut=True)\n",
        "print_gradients(model_with_shortcut, sample_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrHA5tXV41sO"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self,cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut\n",
        "\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3UTjyYhBipD",
        "outputId": "15c2fc75-1042-4a76-a854-c1ab8e22d68d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input shape: torch.Size([2, 3, 768])\n",
            "output shape: torch.Size([2, 3, 768])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "model = torch.rand(2, 4, 768)\n",
        "block = TransformerBlock(GPT_CONFIG_124M)\n",
        "output = block(x)\n",
        "print(\"input shape:\",x.shape)\n",
        "print(\"output shape:\",output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgUQHVNQH9jv"
      },
      "source": [
        "GPT Architecture part 6: Entire GPT Model Architecture implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuo_l3trHsoL"
      },
      "outputs": [],
      "source": [
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdH7khBGIJ7r",
        "outputId": "2e112018-6c7b-4dd7-e227-941018a8f3f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INPUT batch:\n",
            " tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n",
            "OUTPUT SHAPE torch.Size([2, 4, 50257])\n",
            "tensor([[[ 0.1381,  0.0077, -0.1963,  ..., -0.0222, -0.1060,  0.1717],\n",
            "         [ 0.3865, -0.8408, -0.6564,  ..., -0.5163,  0.2369, -0.3357],\n",
            "         [ 0.6989, -0.1829, -0.1631,  ...,  0.1472, -0.6504, -0.0056],\n",
            "         [-0.4290,  0.1669, -0.1258,  ...,  1.1579,  0.5303, -0.5549]],\n",
            "\n",
            "        [[ 0.1094, -0.2894, -0.1467,  ..., -0.0557,  0.2911, -0.2824],\n",
            "         [ 0.0882, -0.3552, -0.3527,  ...,  1.2930,  0.0053,  0.1898],\n",
            "         [ 0.6091,  0.4702, -0.4094,  ...,  0.7688,  0.3787, -0.1974],\n",
            "         [-0.0612, -0.0737,  0.4751,  ...,  1.2463, -0.3834,  0.0609]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "out = model(batch)\n",
        "print(\"INPUT batch:\\n\",batch)\n",
        "print(\"OUTPUT SHAPE\",out.shape)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDdjqrfmL21p",
        "outputId": "9da05b56-0e63-42f0-f173-8330a05068c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters:  163,009,536\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params: ,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GibBW34DMRmE",
        "outputId": "c4f1fc4b-5f5f-4b35-c505-563be8055b74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token embedding layer shape: torch.Size([50257, 768])\n",
            "output layer shape: torch.Size([50257, 768])\n"
          ]
        }
      ],
      "source": [
        "print(\"token embedding layer shape:\",model.tok_emb.weight.shape)\n",
        "print(\"output layer shape:\",model.out_head.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqxkOwSsMunE",
        "outputId": "d43c0337-b611-49d1-82e7-63ab7bd5f9a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trainable parameters considering weight tying:  124,412,160\n"
          ]
        }
      ],
      "source": [
        "# removing the o/p layer parameter count from the total parameter count\n",
        "total_params_gpt2 = total_params - sum(p.numel() for p in model.out_head.parameters())\n",
        "print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2: ,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlkMGBb_Neux",
        "outputId": "f7ea25be-226f-48ca-b53c-4c0b1995e2e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total size of the model parameters:  621.83 MB\n"
          ]
        }
      ],
      "source": [
        "total_size_bytes = total_params * 4\n",
        "total_size_mb = total_size_bytes / (1024*1024)\n",
        "print(f\"Total size of the model parameters: {total_size_mb: .2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awjmAwPWDsqo"
      },
      "source": [
        "GPT Architecture part 7: Generating text from o/p tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWZc3qOzNyCp"
      },
      "outputs": [],
      "source": [
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    # idx is (batch, n_tokens) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "        # crop current context if it exceeds the supported context size\n",
        "        # eg: if LLM supports only 5 tokens and the contex size is 10\n",
        "        # then only last 5 token are used as context\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        # get the predictions\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)  ## (batch, n_tokens, voacb_size)\n",
        "\n",
        "        # focus only on the last time step\n",
        "        # (batch, n_tokens, vocab_szie) becomes (batch, vocab_size)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # apply softmax to get probabilities\n",
        "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
        "\n",
        "        # get the index of the vocab entry with the highest prob value\n",
        "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
        "\n",
        "        # append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9ya1CGiHDaC",
        "outputId": "01665dd1-98e0-43f2-bfbf-6b49b1669bcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoded: [15496, 11, 314, 716, 220]\n",
            "encoded_tensor.shape: torch.Size([1, 5])\n"
          ]
        }
      ],
      "source": [
        "start_context = \"Hello, I am \"\n",
        "encoded = tokenizer.encode(start_context)\n",
        "print(\"encoded:\",encoded)\n",
        "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "print(\"encoded_tensor.shape:\",encoded_tensor.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iDRu3apcA43",
        "outputId": "61de0f6a-f16c-4297-9d72-5e52ef1f3d19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output: tensor([[15496,    11,   314,   716,   220, 13966, 21091, 35022, 33648,  5924,\n",
            "         42740, 43832, 40041, 26479, 36792]])\n",
            "output length: 15\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "out = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=encoded_tensor,\n",
        "    max_new_tokens=10,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "print(\"output:\",out)\n",
        "print(\"output length:\",len(out[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbXxha-Nc5O2",
        "outputId": "89205017-7efb-4645-96e0-3448280b88fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, I am occジ rubbishGamer experiencedugenDbLair filmmaker ;;\n"
          ]
        }
      ],
      "source": [
        "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
        "print(decoded_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59XeROTxm5TN"
      },
      "source": [
        "Using GPT to generate text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzPjmxQ-ot0Z"
      },
      "source": [
        "Calculating the text generation loss: cross-entropy and perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVkuHAMamJDR"
      },
      "outputs": [],
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,      # vocabulary size\n",
        "    \"context_length\": 256,   # context length shortened (orig: 1024)\n",
        "    \"emb_dim\": 768,           # embedding dimension\n",
        "    \"n_heads\": 12,            # no. of attention heads\n",
        "    \"n_layers\": 12,           # no. of layers\n",
        "    \"drop_rate\": 0.1,         # dropout rate\n",
        "    \"qkv_bias\": False         # query-key-value bias\n",
        "}\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.eval();          # disable dropout during inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFb-yRjZmIxF",
        "outputId": "f5d70e5a-3b0b-4a65-f4d0-d9bce49ba338"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output text:\n",
            " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "    encoded_tensor =  torch.tensor(encoded).unsqueeze(0)\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0)\n",
        "    return tokenizer.decode(flat.tolist())\n",
        "\n",
        "start_context = \"Every effort moves you\"\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(start_context, tokenizer),\n",
        "    max_new_tokens=10,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nPCBYBnomEB"
      },
      "outputs": [],
      "source": [
        "inputs = torch.tensor(([16833, 3626, 6100],   # every effort moves\n",
        "                       [40,    1107, 588]))   # I really like\n",
        "\n",
        "targets = torch.tensor(([3626, 6100, 345],    # effort moves you\n",
        "                        [1107, 588,  11311])) # really like chocolate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWEvyFQLpmTC"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    logits = model(inputs)\n",
        "\n",
        "probas = torch.softmax(logits, dim=-1)\n",
        "print(probas.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9vHCE5yrm-1"
      },
      "outputs": [],
      "source": [
        "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
        "print(\"token ids:\\n\",token_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t23ugrL9sRQO"
      },
      "outputs": [],
      "source": [
        "print(f\"targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
        "print(f\"targets batch 2: {token_ids_to_text(token_ids[0].flatten(),tokenizer)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GHwMYm3wi5P"
      },
      "source": [
        "Cross-entropy loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59ZCAlJeuOpS"
      },
      "outputs": [],
      "source": [
        "text_idx = 0\n",
        "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
        "print(\"Text 1:\", target_probas_1)\n",
        "\n",
        "text_idx = 1\n",
        "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
        "print(\"Text 2:\", target_probas_2)\n",
        "# p11, p12, p13\n",
        "# p21, p22, p23"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "395aPVnPw7ug"
      },
      "outputs": [],
      "source": [
        "# compute logarithm of all token probabilities\n",
        "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
        "print(log_probas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xScCx5QExv-n"
      },
      "outputs": [],
      "source": [
        "# calculate the avg probability for each token\n",
        "avg_log_probas = torch.mean(log_probas)\n",
        "print(avg_log_probas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijcyfXwmyB2k"
      },
      "outputs": [],
      "source": [
        "meg_avg_log_probas = avg_log_probas * -1\n",
        "print(meg_avg_log_probas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIL_MYgIyMZQ"
      },
      "outputs": [],
      "source": [
        "logits_flat = logits.flatten(0,1)\n",
        "targets_flat = targets.flatten()\n",
        "\n",
        "print(\"flattened logits\",logits_flat.shape)\n",
        "print(\"flattened targets\",targets_flat.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsFfbI7KydOx"
      },
      "outputs": [],
      "source": [
        "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e7VC1fHy5Nf"
      },
      "source": [
        "Perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZ_JtugFyl_Z"
      },
      "outputs": [],
      "source": [
        "perplexity = torch.exp(loss)\n",
        "print(perplexity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szfecLYEzFiu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}