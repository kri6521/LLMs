{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells" : [
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader(\"articles\").load_data()"
      ],
      "metadata": {
        "id": "YVxo8BVOl67Y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(documents))\n",
        "for doc in documents:\n",
        "    if \"Member-only story\" in doc.text:\n",
        "        documents.remove(doc)\n",
        "        continue\n",
        "\n",
        "    if \"The Data Entrepreneurs\" in doc.text:\n",
        "        documents.remove(doc)\n",
        "\n",
        "    if \" min read\" in doc.text:\n",
        "        documents.remove(doc)\n",
        "\n",
        "print(len(documents))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXn4R6sLl-Ng",
        "outputId": "5902904d-835b-488c-ae6d-0390127d97b9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "71\n",
            "61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# store docs into vector DB\n",
        "index = VectorStoreIndex.from_documents(documents)"
      ],
      "metadata": {
        "id": "oFjkfe1ImFMx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set number of docs to retreive\n",
        "top_k = 3\n",
        "\n",
        "# configure retriever\n",
        "retriever = VectorIndexRetriever(\n",
        "    index=index,\n",
        "    similarity_top_k=top_k,\n",
        ")\n",
        "\n",
        "\n",
        "# assemble query engine\n",
        "query_engine = RetrieverQueryEngine(\n",
        "    retriever=retriever,\n",
        "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.5)],\n",
        ")"
      ],
      "metadata": {
        "id": "KhBRTBzPmFzZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieve Relevant docs"
      ],
      "metadata": {
        "id": "6_FUuZBztF0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# query documents\n",
        "query = \"What is fat-tailedness?\"\n",
        "response = query_engine.query(query)\n",
        "\n",
        "\n",
        "# reformat response\n",
        "context = \"Context:\\n\"\n",
        "for i in range(top_k):\n",
        "    context = context + response.source_nodes[i].text + \"\\n\\n\"\n",
        "\n",
        "print(context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ka3FbnSmH_Z",
        "outputId": "2e3c241e-a7be-4908-c135-476821d9fb4f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context:\n",
            "Some of the controversy might be explained by the observation that log-\n",
            "normal distributions behave like Gaussian for low sigma and like Power Law\n",
            "at high sigma [2].\n",
            "However, to avoid controversy, we can depart (for now) from whether some\n",
            "given data fits a Power Law or not and focus instead on fat tails.\n",
            "Fat-tailedness — measuring the space between Mediocristan\n",
            "and Extremistan\n",
            "Fat Tails are a more general idea than Pareto and Power Law distributions.\n",
            "One way we can think about it is that “fat-tailedness” is the degree to which\n",
            "rare events drive the aggregate statistics of a distribution. From this point of\n",
            "view, fat-tailedness lives on a spectrum from not fat-tailed (i.e. a Gaussian) to\n",
            "very fat-tailed (i.e. Pareto 80 – 20).\n",
            "This maps directly to the idea of Mediocristan vs Extremistan discussed\n",
            "earlier. The image below visualizes different distributions across this\n",
            "conceptual landscape [2].\n",
            "\n",
            "print(\"mean kappa_1n = \" + str(np.mean(kappa_dict[filename])))\n",
            "    print(\"\")\n",
            "Mean κ (1,100) values from 1000 runs for each dataset. Image by author.\n",
            "These more stable results indicate Medium followers are the most fat-tailed,\n",
            "followed by LinkedIn Impressions and YouTube earnings.\n",
            "Note: One can compare these values to Table III in ref [3] to better understand each\n",
            "κ  value. Namely, these values are comparable to a Pareto distribution with α\n",
            "between 2 and 3.\n",
            "Although each heuristic told a slightly different story, all signs point toward\n",
            "Medium followers gained being the most fat-tailed of the 3 datasets.\n",
            "Conclusion\n",
            "While binary labeling data as fat-tailed (or not) may be tempting, fat-\n",
            "tailedness lives on a spectrum. Here, we broke down 4 heuristics for\n",
            "quantifying how fat-tailed data are.\n",
            "\n",
            "Pareto, Power Laws, and Fat Tails\n",
            "What they don’t teach you in statistics\n",
            "towardsdatascience.com\n",
            "Although Pareto (and more generally power law) distributions give us a\n",
            "salient example of fat tails, this is a more general notion that lives on a\n",
            "spectrum ranging from thin-tailed (i.e. a Gaussian) to very fat-tailed (i.e.\n",
            "Pareto 80 – 20).\n",
            "The spectrum of Fat-tailedness. Image by author.\n",
            "This view of fat-tailedness provides us with a more flexible and precise way of\n",
            "categorizing data than simply labeling it as a Power Law (or not). However,\n",
            "this begs the question: how do we define fat-tailedness?\n",
            "4 Ways to Quantify Fat Tails\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the fine tuning model"
      ],
      "metadata": {
        "id": "h4Y6VrPCn8d0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load fine-tuned model from hub\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             device_map=\"auto\",\n",
        "                                             trust_remote_code=False,\n",
        "                                             revision=\"main\")\n",
        "\n",
        "config = PeftConfig.from_pretrained(\"shawhin/shawgpt-ft\")\n",
        "model = PeftModel.from_pretrained(model, \"shawhin/shawgpt-ft\")\n",
        "\n",
        "# load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533,
          "referenced_widgets": [
            "46dfe69d40a8463ea549d2d4803bc5a0",
            "a24495ad85bd4006ab0d62e5b01a1f36",
            "88c081f100b4434ea1c6c33b0e86d107",
            "c73642edb1ff42b2a4f8c9042047ccd5",
            "68861e6d01de4e108361bd0bc566f96b",
            "c9da68482c8049c393a6db7fee16f106",
            "4ce44ce66a4d4732b6b42e723b2f64e8",
            "457bc81407e04397a6599b6ec68d0689",
            "e9d12e37351d4c80ab3dcc5511888171",
            "8b83870987174e71be260d20e7105e23",
            "77000388ece94f40bd50a9ca934c8af5",
            "033359d8f36c4a10991c39fc3a62d393",
            "69ff025163634f01a757227f27265381",
            "b0ec26ffda744215b138705c52282bd5",
            "201466d9d4b14166bbe6c7a39696435e",
            "c0712bf25fbe45eeaac42dcf9aa42cdd",
            "a30f4ff4dcf3458f97cc75dad17c0955",
            "0de966ab75764fdfa9759bd4b47c7e4c",
            "4035e06d91924d32b24a0338f87af51d",
            "1f86f96245ae4be38bcc2018dbeed9af",
            "550d594462904ff08fc35cb91799f1cb",
            "8f0e73e1149c48a485e4bb8978c3c5ea",
            "a19f944130ca441c9e77933604222dbc",
            "c14abf47b1e04e089133d4a6f0538043",
            "158df2b9153844c38a31aac8c5198538",
            "67756cd296b84b088c0abc00e0517f9a",
            "a1eac06ecd0e4636b5b7e9b4fffe2c0a",
            "dd307fa1237349a4bd9a0de5887f31c4",
            "1c150cad3300499cb433fced078d9818",
            "d78877ef43374815941b8c2f1ff2662d",
            "aba8c7534b3e4c9d83ed0fa265422cac",
            "366f8d1de6b3445eb1f1318fe87fe62c",
            "6779008ef3fd4c7387ae076e0afd648e",
            "3fbf44f4143740eb85c58d9db32534b5",
            "cefab0b2198d405bb4a315d8d89b49ea",
            "6f2289b7b0d542ef9f073d4a8e321704",
            "b5e53f6d110c413f8a67e07329dce0d5",
            "95f52948f3a840ee802e2db08f8977bb",
            "a3da09a05953417fabcb2343f30ec445",
            "a8bd4fc5388440a7a5f7ac74329bcd68",
            "c329fe1f257c418b8e130e8a1e2ef7db",
            "e958adf775a74f54b37bc3e7ace24607",
            "173e2741ce0e4206b7f307942af12022",
            "65fde14cbaf84640ac4d70142919e474",
            "ce7f93a399b042389cd8745669a80874",
            "da7627aac1bd4b46a6c302b585e2fe2f",
            "93e99e894719408a8b98d6a4baff3076",
            "f5a401a6d2704382bdec085682c532d5",
            "3a56fb536eb84ce98a08bf77455ed594",
            "931e9c7a1d804df8aa344f3f681061d6",
            "2f6c61b72e3c47b0be8d9c0f1612f86f",
            "bb85f4a389ed480e8b11add10ccb7926",
            "2f4666f2fff749cbb85e1f11c50358c4",
            "11f14605d4164750a160babe7a3caf1f",
            "333a8eedad7646b7b94513f01838da3f",
            "eb0e695d65504312b9a29a38b9a7a274",
            "e9580a7e936b4c37bac0953886118c30",
            "1df6c82501234567b4719d218de96095",
            "52e92e11dc6c421cba030fc6bf3287b6",
            "6b320f507dd24e4b9054facdac6e6459",
            "94ce60599a8f457c9ddc341a5df822c9",
            "a23c43151b7f488dbbadc8090e7bfb58",
            "6a80bbdcfb0f4202935d85553df5dcd0",
            "522aecbd8e044869b178d47903006597",
            "e84e038aeb9e47ca8c4b82ff39c999a9",
            "9e39682c969244d0baac7e6442b44648",
            "6ee051531b3a448db08569292a690ce5",
            "4645a01efdeb441c83a519b62e7ab28b",
            "b2b51ed00c7742178ba1dd7fa4a80f38",
            "14cc2c2cd3a943e4a8efcdcf865d514b",
            "18f28483751a4126b9940778548d0135",
            "917b26fbf8d24823aece6708368ec8a6",
            "f9b4b307a124454faf63b9e2fd2a29db",
            "fadfcdd4b18f418098f52fa49d4428d9",
            "a98510f3bb4747bc84250d187b9fc472",
            "a2acb562edd1498c854c673e4b4eb19d",
            "d75f65b23e274229a2f00e288d10765b",
            "957deedb903a491fafec8a272d7b0b19",
            "0b3f5a27a9ed4e94970d572ef9fa88c7",
            "cd37975a3bf046fb891777cb63d641ee",
            "69af69ab1b4e46aa98d813ee488bb701",
            "354612896d5d45a1a26f20654692bcb4",
            "b179a74de7a04d219e6ec7d86913d911",
            "0b937468aa0f4a1ea87261a9392cfc84",
            "b3b8858fd9fe4d37ba731c72322f7be9",
            "98eb94f1662149e38c8971476ee4d20b",
            "8f5367a1b1bc48eeab7007e234755aba",
            "51a23203ba8d45cc87fbffaa1a9fbfc0",
            "7a58834b02994f3494f73413868cd851",
            "f5949a54256c4758a0c712ef3640a72a",
            "36a594225b3b4387babc9cf2cfd153fe",
            "8502f5fb3d0d4f79a38a172f7a64c960",
            "e21356c610164791b39a8fffb68c0628",
            "a0289f2af2684318877efbab0e0ee1bb",
            "ff5d295a1bd34bfeb80813effb841e7c",
            "d5cd4cafda8a4402b0c3fb177e00aefc",
            "b70ef6adb0544d3bbd6e67d4771e75b8",
            "e4d209b5662f4a1bb82a1c19cb5df55b",
            "815b27014c4b42b88e826e173a27b4a8"
          ]
        },
        "id": "TBF7EsRamMX0",
        "outputId": "96d96be3-ad3e-4dbe-da08-a9f16881e615"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/963 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46dfe69d40a8463ea549d2d4803bc5a0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  @custom_bwd\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd(cast_inputs=torch.float16)\n",
            "WARNING:auto_gptq.nn_modules.qlinear.qlinear_cuda:CUDA extension not installed.\n",
            "WARNING:auto_gptq.nn_modules.qlinear.qlinear_cuda_old:CUDA extension not installed.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/4.16G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "033359d8f36c4a10991c39fc3a62d393"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
            "Some weights of the model checkpoint at TheBloke/Mistral-7B-Instruct-v0.1-GPTQ were not used when initializing MistralForCausalLM: {'model.layers.1.self_attn.v_proj.bias', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.10.mlp.up_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.28.mlp.down_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.26.mlp.down_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.0.mlp.up_proj.bias', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.31.mlp.up_proj.bias', 'model.layers.29.mlp.down_proj.bias', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.24.mlp.down_proj.bias', 'model.layers.2.mlp.up_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.19.mlp.up_proj.bias', 'model.layers.26.mlp.up_proj.bias', 'model.layers.16.mlp.down_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.20.mlp.up_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.30.mlp.down_proj.bias', 'model.layers.15.mlp.up_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.3.mlp.up_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.22.mlp.up_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.1.mlp.up_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.8.mlp.up_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.9.mlp.up_proj.bias', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.23.mlp.up_proj.bias', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.30.mlp.up_proj.bias', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.19.mlp.down_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.31.mlp.down_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.25.mlp.up_proj.bias', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.13.mlp.up_proj.bias', 'model.layers.24.mlp.up_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.14.mlp.up_proj.bias', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.7.mlp.down_proj.bias', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.7.mlp.up_proj.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.0.mlp.down_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.4.mlp.up_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.17.mlp.up_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.25.mlp.down_proj.bias', 'model.layers.29.mlp.up_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.11.mlp.up_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.27.mlp.up_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.27.mlp.down_proj.bias', 'model.layers.5.mlp.up_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.28.mlp.up_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.30.self_attn.q_proj.bias'}\n",
            "- This IS expected if you are initializing MistralForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing MistralForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a19f944130ca441c9e77933604222dbc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "adapter_config.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3fbf44f4143740eb85c58d9db32534b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/8.40M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce7f93a399b042389cd8745669a80874"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb0e695d65504312b9a29a38b9a7a274"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ee051531b3a448db08569292a690ce5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "957deedb903a491fafec8a272d7b0b19"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a58834b02994f3494f73413868cd851"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt (no context)\n",
        "intstructions_string = f\"\"\"ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. \\\n",
        "It reacts to feedback aptly and ends responses with its signature '–ShawGPT'. \\\n",
        "ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, \\\n",
        "thus keeping the interaction natural and engaging.\n",
        "\n",
        "Please respond to the following comment.\n",
        "\"\"\"\n",
        "prompt_template = lambda comment: f'''[INST] {intstructions_string} \\n{comment} \\n[/INST]'''\n",
        ""
      ],
      "metadata": {
        "id": "tfKPUafFmNIv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comment = \"What is fat-tailedness?\"\n",
        "\n",
        "prompt = prompt_template(comment)\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhRxPS0OmSNC",
        "outputId": "6b7ec495-2ece-4ebd-a077-340a0eedcc79"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INST] ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. It reacts to feedback aptly and ends responses with its signature '–ShawGPT'. ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.\n",
            "\n",
            "Please respond to the following comment.\n",
            " \n",
            "What is fat-tailedness? \n",
            "[/INST]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=280)\n",
        "\n",
        "print(tokenizer.batch_decode(outputs)[0])\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Og3KsReXmS1b",
        "outputId": "452544f6-14be-457b-c859-2cea9883b805"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> [INST] ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. It reacts to feedback aptly and ends responses with its signature '–ShawGPT'. ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.\n",
            "\n",
            "Please respond to the following comment.\n",
            " \n",
            "What is fat-tailedness? \n",
            "[/INST] Hi there! Fat-tailedness refers to the property of a distribution where the tails are fatter or heavier than a normal distribution. In other words, the probability of observing extreme values (outliers) is higher in a fat-tailed distribution compared to a normal distribution. Fat-tailedness is often associated with power-law distributions, which are commonly found in nature and finance. Thanks for asking! –ShawGPT</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt (with context)\n",
        "prompt_template_w_context = lambda context, comment: f\"\"\"[INST]ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. \\\n",
        "It reacts to feedback aptly and ends responses with its signature '–ShawGPT'. \\\n",
        "ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, \\\n",
        "thus keeping the interaction natural and engaging.\n",
        "\n",
        "{context}\n",
        "Please respond to the following comment. Use the context above if it is helpful.\n",
        "\n",
        "{comment}\n",
        "[/INST]\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "IAmXGDC8mXvE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = prompt_template_w_context(context, comment)\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=280)\n",
        "\n",
        "print(tokenizer.batch_decode(outputs)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QItCe-BnmYcy",
        "outputId": "00d8e4cc-eec6-44b2-e6f1-32314ea98f35"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> [INST]ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. It reacts to feedback aptly and ends responses with its signature '–ShawGPT'. ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.\n",
            "\n",
            "Context:\n",
            "Some of the controversy might be explained by the observation that log-\n",
            "normal distributions behave like Gaussian for low sigma and like Power Law\n",
            "at high sigma [2].\n",
            "However, to avoid controversy, we can depart (for now) from whether some\n",
            "given data fits a Power Law or not and focus instead on fat tails.\n",
            "Fat-tailedness — measuring the space between Mediocristan\n",
            "and Extremistan\n",
            "Fat Tails are a more general idea than Pareto and Power Law distributions.\n",
            "One way we can think about it is that “fat-tailedness” is the degree to which\n",
            "rare events drive the aggregate statistics of a distribution. From this point of\n",
            "view, fat-tailedness lives on a spectrum from not fat-tailed (i.e. a Gaussian) to\n",
            "very fat-tailed (i.e. Pareto 80 – 20).\n",
            "This maps directly to the idea of Mediocristan vs Extremistan discussed\n",
            "earlier. The image below visualizes different distributions across this\n",
            "conceptual landscape [2].\n",
            "\n",
            "print(\"mean kappa_1n = \" + str(np.mean(kappa_dict[filename])))\n",
            "    print(\"\")\n",
            "Mean κ (1,100) values from 1000 runs for each dataset. Image by author.\n",
            "These more stable results indicate Medium followers are the most fat-tailed,\n",
            "followed by LinkedIn Impressions and YouTube earnings.\n",
            "Note: One can compare these values to Table III in ref [3] to better understand each\n",
            "κ  value. Namely, these values are comparable to a Pareto distribution with α\n",
            "between 2 and 3.\n",
            "Although each heuristic told a slightly different story, all signs point toward\n",
            "Medium followers gained being the most fat-tailed of the 3 datasets.\n",
            "Conclusion\n",
            "While binary labeling data as fat-tailed (or not) may be tempting, fat-\n",
            "tailedness lives on a spectrum. Here, we broke down 4 heuristics for\n",
            "quantifying how fat-tailed data are.\n",
            "\n",
            "Pareto, Power Laws, and Fat Tails\n",
            "What they don’t teach you in statistics\n",
            "towardsdatascience.com\n",
            "Although Pareto (and more generally power law) distributions give us a\n",
            "salient example of fat tails, this is a more general notion that lives on a\n",
            "spectrum ranging from thin-tailed (i.e. a Gaussian) to very fat-tailed (i.e.\n",
            "Pareto 80 – 20).\n",
            "The spectrum of Fat-tailedness. Image by author.\n",
            "This view of fat-tailedness provides us with a more flexible and precise way of\n",
            "categorizing data than simply labeling it as a Power Law (or not). However,\n",
            "this begs the question: how do we define fat-tailedness?\n",
            "4 Ways to Quantify Fat Tails\n",
            "\n",
            "\n",
            "Please respond to the following comment. Use the context above if it is helpful.\n",
            "\n",
            "What is fat-tailedness?\n",
            "[/INST]\n",
            "\n",
            "–ShawGPT\n",
            "\n",
            "Fat-tailedness is a measure of the degree to which rare events drive the aggregate statistics of a distribution. It is a more general idea than Pareto and Power Law distributions, and it lives on a spectrum ranging from thin-tailed (i.e., a Gaussian) to very fat-tailed (i.e., Pareto 80 – 20). In other words, fat-tailedness is the extent to which the data deviates from a normal distribution, with rare events having a disproportionate impact on the overall statistics.</s>\n"
          ]
        }
      ]
    }
  ]
}
